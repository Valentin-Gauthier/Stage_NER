{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8e22be",
   "metadata": {},
   "source": [
    "# NER Pipeline\n",
    "\n",
    "...\n",
    "## ðŸ“„ Overview\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ›  Dependencies\n",
    "\n",
    "To run this notebook, make sure you have the following:\n",
    "\n",
    "### âœ… Requirements\n",
    "\n",
    "- **Python 3.8 or higher**\n",
    "- **Jupyter Notebook** or **JupyterLab** (to run `.ipynb` files)\n",
    "\n",
    "Install Jupyter Notebook:\n",
    "```bash\n",
    "pip install notebook\n",
    "```\n",
    "### ðŸ“¦ Python Packages\n",
    "```bash\n",
    "python -m spacy download fr_core_news_sm\n",
    "python -m spacy download fr_core_news_md\n",
    "python -m spacy download fr_core_news_lg\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33ff51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- IMPORTS ----------------\n",
    "import time\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from spacy import displacy\n",
    "from time import sleep\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea0b2fc",
   "metadata": {},
   "source": [
    "# Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data in : 2.11s\n"
     ]
    }
   ],
   "source": [
    "def load_excel_file(file_path:str):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        Load a Excel file into a DataFrame\n",
    "\n",
    "        Parameters:\n",
    "            file_path (str): The file path\n",
    "\n",
    "        Returns:\n",
    "            df (pd.Dataframe) : the result of Excel file\n",
    "    \"\"\"\n",
    "\n",
    "    # Start Chrono\n",
    "    start_chrono = time.time()\n",
    "\n",
    "    # Open the Excel file\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Clean\n",
    "    df.drop(df.columns[0], axis=1, inplace=True) # Drop the Column\n",
    "    #print(df.isnull().sum())\n",
    "    df = df.fillna('') # clean the missing description\n",
    "    \n",
    "    # End Chrono\n",
    "    end_chrono = time.time()\n",
    "    chrono = end_chrono - start_chrono\n",
    "    print(f\"Load data in : {chrono:.2f}s\")\n",
    "\n",
    "    return df\n",
    "\n",
    "#df = load_excel_file(\"ressources/20231101_raw.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428dfb15",
   "metadata": {},
   "source": [
    "# Use Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the Spacy modele \n",
    "#NER = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2fc0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_spacy(df, NER):\n",
    "    \"\"\"\n",
    "        Apply Spacy\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.Dataframe): the dataframe \n",
    "    \"\"\"\n",
    "\n",
    "    ner_spacy = [] # (\"text_entite\", \"label\")\n",
    "    ner_spacy_id = [] # (\"start\", \"end\")\n",
    "\n",
    "    # Start Chrono\n",
    "    start_chrono = time.time()\n",
    "    # foreach row , use spacy on the description\n",
    "    for idx, row in df.iterrows():\n",
    "        desc = str(row[\"desc\"]) # extract the description\n",
    "        entities = NER(desc) # use spacy \n",
    "        entities_founds = set() # Manage duplicates automatically\n",
    "        entities_locations = []\n",
    "\n",
    "        for word in entities.ents:\n",
    "            entities_founds.add((word.text, word.label_))\n",
    "            entities_locations.append((word.start, word.end))\n",
    "\n",
    "        ner_spacy.append(entities_founds)\n",
    "        ner_spacy_id.append(entities_locations)\n",
    "\n",
    "    # End Chrono\n",
    "    end_chrono = time.time()\n",
    "    chrono = end_chrono - start_chrono\n",
    "    print(f\"Apply Spacy in : {chrono:.2f}s\")\n",
    "\n",
    "    return (ner_spacy, ner_spacy_id)\n",
    "\n",
    "# ner_spacy, ner_spacy_id = apply_spacy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "158aa3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ner_dataframe(df, ner_spacy, ner_spacy_id, NER):\n",
    "    \"\"\"\n",
    "    Build a new DataFrame from the original DataFrame and precomputed NER outputs.\n",
    "    \n",
    "    The original DataFrame must contain at least two columns: 'titles' and 'desc'.\n",
    "    For each row, the function uses the provided NER results (entity text/label and their corresponding indices)\n",
    "    to extract a portion of the description text with some context around each entity.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The original DataFrame containing a 'titles' column and a 'desc' (description) column.\n",
    "        ner_spacy (list): A list (one per row) of lists of tuples, each tuple containing the entity text and its label.\n",
    "                          For example: [[(\"Barack Obama\", \"PERSON\"), (\"Washington\", \"GPE\")], ...]\n",
    "        ner_spacy_id (list): A list (one per row) of lists of tuples, each tuple containing the start and end indices of the entity.\n",
    "                             For example: [[(0, 12), (20, 30)], ...]\n",
    "                             \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the following columns:\n",
    "                      - \"titles\": the original title repeated for each extracted entity.\n",
    "                      - \"NER\": the extracted entity text.\n",
    "                      - \"NER_label\": the label/type of the entity.\n",
    "                      - \"desc\": a snippet of the original description around the entity.\n",
    "    \"\"\"\n",
    "\n",
    "    start_chrono = time.time()\n",
    "\n",
    "    # Convert the 'titles' and 'desc' columns to lists\n",
    "    titles_list = df[\"titles\"].tolist()\n",
    "    descriptions_list = df[\"desc\"].tolist()\n",
    "    \n",
    "    # Initialize lists to store the new data\n",
    "    title_list = []\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    location_list = []\n",
    "    file_id_list = []\n",
    "    \n",
    "    # Loop through each entry in the original DataFrame (by index)\n",
    "    for i in range(len(titles_list)):\n",
    "        current_desc = descriptions_list[i]\n",
    "        # Process the description using the NER function to obtain a full processed document.\n",
    "        # Assume that NER() returns an object where converting it to a string gives its full text.\n",
    "        current_NER_doc = NER(current_desc)\n",
    "        \n",
    "        # Get the precomputed NER entities and their indices for this row\n",
    "        current_NER = ner_spacy[i]\n",
    "        indices = ner_spacy_id[i]\n",
    "        \n",
    "        # Get the title associated with the current row\n",
    "        current_title = titles_list[i]\n",
    "        \n",
    "        # Loop over each entity and its corresponding indices\n",
    "        for entity, idx in zip(current_NER, indices):\n",
    "            entity_text, entity_label = entity\n",
    "            \n",
    "            # Append data to the corresponding list\n",
    "            title_list.append(current_title)\n",
    "            text_list.append(entity_text)\n",
    "            label_list.append(entity_label)\n",
    "            file_id_list.append(i)\n",
    "            \n",
    "            # Extract a snippet from the full document text around the entity for context.\n",
    "            # If the entity starts near the beginning, we extract from the start.\n",
    "            if idx[0] < 10:\n",
    "                # If entity is close to the end of the document, include all text until the end.\n",
    "                if idx[1] + 8 > len(current_NER_doc):\n",
    "                    location_list.append(current_NER_doc)\n",
    "                else:\n",
    "                    location_list.append(current_NER_doc[:idx[1] + 8])\n",
    "            else:\n",
    "                # Else extract 10 characters before the entity start.\n",
    "                # If the entity is near the end, extract until the end.\n",
    "                if idx[1] + 8 > len(current_NER_doc):\n",
    "                    location_list.append(current_NER_doc[idx[0] - 10:])\n",
    "                else:\n",
    "                    location_list.append(current_NER_doc[idx[0] - 10: idx[1] + 8])\n",
    "    \n",
    "    # Build a new DataFrame using the collected lists.\n",
    "    new_df = pd.DataFrame(zip(title_list, text_list, label_list, location_list, file_id_list))\n",
    "    \n",
    "    # Rename the columns to be more informative.\n",
    "    new_df.rename(columns={0: \"titles\", 1: \"NER\", 2: \"NER_label\", 3: \"desc\", 4:\"file_id\"}, inplace=True)\n",
    "\n",
    "    end_chrono = time.time()\n",
    "    chrono = end_chrono - start_chrono\n",
    "    print(f\"Generate Spacy DataFrame in : {chrono:.2f}s\")\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# df_ner = build_ner_dataframe(df, ner_spacy, ner_spacy_id)\n",
    "# df_ner.head()\n",
    "#df_ner.to_excel(\"Result/xlsx/DataFrame_Spacy.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9dd386",
   "metadata": {},
   "source": [
    "# CasEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d155ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_files(df):\n",
    "    start_chrono = time.time()\n",
    "\n",
    "    # Generate text file \n",
    "    text_location = \"Result/Corpus/All.txt\"\n",
    "    \n",
    "    with open(text_location, 'w', encoding=\"utf-8\") as f:\n",
    "        for idx, desc in enumerate(df[\"desc\"]):\n",
    "            f.write(f'<doc id=\"{idx}\">')\n",
    "            f.write(str(desc))\n",
    "            f.write('</doc>\\n')\n",
    "\n",
    "    end_chrono = time.time()\n",
    "    chrono = end_chrono - start_chrono\n",
    "    print(f\"Generate text files in : {chrono:.2f}s\")\n",
    "\n",
    "#generate_text_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab275297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CasEN\n",
    "# casen_ipynb_path = \"../CasEN_fr.2.0/CasEN.ipynb\"\n",
    "# get_ipython().run_line_magic('run', str(casen_ipynb_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17275a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_tree(soup_doc: BeautifulSoup, doc_id: int):\n",
    "    \"\"\"\n",
    "    Ne remonte que les entitÃ©s ayant un grf et sans ancÃªtre grf,\n",
    "    et stocke pour chacune :\n",
    "      - file_id, tag, text, grf (main)\n",
    "      - second_graph  = grf du 1er enfant direct\n",
    "      - third_graph   = grf du 2Ã¨me enfant direct\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    excluded = {\"s\", \"p\", \"doc\"}\n",
    "\n",
    "    # On parcourt tous les Ã©lÃ©ments taggÃ©s avec grf\n",
    "    for element in soup_doc.find_all(lambda tag: tag.name not in excluded and tag.has_attr(\"grf\")):\n",
    "        # Si un des ancÃªtres a aussi un grf, on l'ignore (c'est un sous-Ã©lÃ©ment)\n",
    "        if any(ancestor.has_attr(\"grf\") for ancestor in element.parents if ancestor.name not in excluded):\n",
    "            continue\n",
    "\n",
    "        main_grf = element[\"grf\"]\n",
    "        # on collecte les grf des enfants directs pour second et third\n",
    "        children = [child for child in element.find_all(recursive=False) if child.has_attr(\"grf\")]\n",
    "        second = children[0][\"grf\"] if len(children) >= 1 else \"\"\n",
    "        third  = children[1][\"grf\"] if len(children) >= 2 else \"\"\n",
    "\n",
    "        entities.append({\n",
    "            \"file_id\":       doc_id,\n",
    "            \"tag\":           element.name,\n",
    "            \"text\":          element.get_text(),\n",
    "            \"grf\":           main_grf,\n",
    "            \"second_graph\":  second,\n",
    "            \"third_graph\":   third\n",
    "        })\n",
    "\n",
    "    return entities\n",
    "\n",
    "def extract_entities_from_file(result_path :str):\n",
    "\n",
    "    with open(result_path, 'r', encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    content = re.sub(r'</?s\\b[^>]*>', '', content)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    all_entities = []\n",
    "\n",
    "    for doc in soup.find_all('doc'):\n",
    "        doc_id = int(doc.attrs.get(\"id\"))\n",
    "        ents = extract_entities_from_tree(doc, doc_id)\n",
    "        all_entities.extend(ents)\n",
    "\n",
    "    return all_entities\n",
    "\n",
    "def casEN_df_single(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a DataFrame of NER results from a single CasEN output file.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    list_entities = extract_entities_from_file(\"Result/CasEN_Result/Res_CasEN_Analyse_synthese_grf/All.result.txt\")\n",
    "    rows = []\n",
    "    for entity in list_entities:\n",
    "        idx = entity[\"file_id\"]\n",
    "        ner_text = entity[\"text\"]\n",
    "        context = get_ner_context(df.loc[idx, \"desc\"], ner_text, window=7)\n",
    "        rows.append({\n",
    "            \"titles\": df.loc[idx, \"titles\"],\n",
    "            \"NER\": ner_text,\n",
    "            \"NER_label\": find_ner_label(entity[\"tag\"]),\n",
    "            \"desc\": context,\n",
    "            \"method\": \"CasEN\",\n",
    "            \"main_graph\": entity[\"grf\"],\n",
    "            \"second_graph\": entity.get(\"second_graph\", \"\"),\n",
    "            \"third_graph\": entity.get(\"third_graph\", \"\"),\n",
    "            \"file_id\": idx\n",
    "        })\n",
    "    df_casen = pd.DataFrame(rows)\n",
    "    print(f\"Built CasEN DataFrame in {time.time() - start:.2f}s\")\n",
    "    return df_casen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acab817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_context(full_desc: str, ner_text: str, window: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a context window around the first match of the NER text in the full descriptionE\n",
    "    \n",
    "    Parameters:\n",
    "        full_desc (str): The full text description.\n",
    "        ner_text (str): The NER text to find in the description.\n",
    "        window (int): Number of words to extract before and after the match\n",
    "\n",
    "    Returns:\n",
    "        str: A snippet of the description centered around the NER match\n",
    "    \"\"\"\n",
    "    # Normalize input for search\n",
    "    full_desc_clean = re.sub(r'[^\\w\\s]', '', full_desc.lower())\n",
    "    ner_clean = re.sub(r'[^\\w\\s]', '', ner_text.lower())\n",
    "\n",
    "    # Tokenize words\n",
    "    words = full_desc.split()\n",
    "    clean_words = full_desc_clean.split()\n",
    "    ner_words = ner_clean.split()\n",
    "\n",
    "    # Try to find full match of all NER words\n",
    "    for i in range(len(clean_words) - len(ner_words) + 1):\n",
    "        if clean_words[i:i+len(ner_words)] == ner_words:\n",
    "            start = max(0, i - window)\n",
    "            end = min(len(words), i + len(ner_words) + window)\n",
    "            return ' '.join(words[start:end])\n",
    "\n",
    "    return full_desc\n",
    "\n",
    "def find_ner_label(tag:str) -> str:\n",
    "    if tag in [\"persname\", \"surname\", \"forename\"]:\n",
    "        return \"PER\"\n",
    "    elif tag in ['placename','geoname', 'adress', 'adrline']:\n",
    "        return \"LOC\"\n",
    "    elif tag in [\"orgname\", \"org\"]:\n",
    "        return \"ORG\"\n",
    "    else:\n",
    "        return \"MISC\"\n",
    "\n",
    "def casEN_df(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Extracts named entities from external CasEN result files and compiles them into a DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame) : DataFrame containing original data \n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame -> result of CasEN analyse in DataFrame format\n",
    "    \n",
    "    \"\"\"\n",
    "    start_chrono = time.time()\n",
    "\n",
    "    titles = df[\"titles\"].to_list()\n",
    "    desc = df[\"desc\"].to_list()\n",
    "\n",
    "    # Make a new DataFrame \n",
    "    df_casEN = pd.DataFrame(columns=[\"titles\", \"NER\", \"NER_label\", \"desc\", \"method\", \"main_graph\", \"second_graph\", \"third_graph\", \"file_id\"])\n",
    "\n",
    "    for index in range(len(titles)):\n",
    "\n",
    "        entities = extract_entities_from_file(f\"Result/CasEN_Result/Res_CasEN_Analyse_synthese_grf/np{index}.result.txt\")\n",
    "\n",
    "        for entity in entities:\n",
    "\n",
    "            # Find every graph\n",
    "            main_graph = entity.get(\"grf\", \"\")\n",
    "            secondary_graph = [value[\"grf\"] for value in entity.values() if isinstance(value, dict) and \"grf\" in value]\n",
    "            # Make desc shorter\n",
    "            ner_text = entity.get(\"text\", \"\")\n",
    "            context_desc = get_ner_context(desc[index], ner_text, 7)\n",
    "\n",
    "            row = {\n",
    "                \"titles\" : titles[index],\n",
    "                \"NER\" : ner_text,\n",
    "                \"NER_label\" : find_ner_label(entity.get(\"tag\")),\n",
    "                \"desc\" : context_desc,\n",
    "                \"method\" : \"CasEN\",\n",
    "                \"main_graph\" : main_graph,\n",
    "                \"second_graph\" : secondary_graph[0] if len(secondary_graph) > 0 else \"\",\n",
    "                \"third_graph\" : secondary_graph[1] if len(secondary_graph) > 1 else \"\",\n",
    "                \"file_id\" : index\n",
    "            }\n",
    "            df_casEN.loc[len(df_casEN)] = row\n",
    "\n",
    "    end_chrono = time.time()\n",
    "    chrono = end_chrono - start_chrono\n",
    "    print(f\"Generate CasEN DataFrame in : {chrono:.2f}s\")\n",
    "\n",
    "    return df_casEN\n",
    "        \n",
    "#df_casEN = casEN_df_single(df)\n",
    "#df_casEN.to_excel(\"Result/xlsx/DataFrame_CasEN.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "436f1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_method(row):\n",
    "    if row['_merge'] == 'both':\n",
    "        return \"intersection\"\n",
    "    elif row['_merge'] == 'left_only':\n",
    "        return row['method_spacy']\n",
    "    elif row['_merge'] == 'right_only':\n",
    "        return row['method_casEN']\n",
    "    else:\n",
    "        return \"Spacy\"\n",
    "\n",
    "def merge_spacy_casEN(df_spacy:pd.DataFrame,df_casEN:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    start_chrono = time.time()\n",
    "\n",
    "    if \"method\" not in df_spacy.columns:\n",
    "        df_spacy[\"method\"] = \"Spacy\"\n",
    "\n",
    "    # Create unique key on (\"titles\", \"NER\", \"NER_label\")\n",
    "    df_spacy[\"key\"] = df_spacy[[\"titles\", \"NER\", \"NER_label\", \"file_id\"]].apply(lambda x: tuple(x), axis=1)\n",
    "    df_casEN[\"key\"] = df_casEN[[\"titles\", \"NER\", \"NER_label\", \"file_id\"]].apply(lambda x: tuple(x), axis=1)\n",
    "\n",
    "    # merge dataframs on the key\n",
    "    merge = pd.merge(df_spacy, df_casEN, on='key', how='outer',suffixes=('_spacy', '_casEN'), indicator=True)\n",
    "\n",
    "    # fix the shared columns\n",
    "    merge[\"titles\"] = merge[\"titles_spacy\"].combine_first(merge[\"titles_casEN\"])\n",
    "    merge['method'] = merge.apply(determine_method, axis=1)\n",
    "    merge[\"NER\"] = merge[\"NER_spacy\"].combine_first(merge[\"NER_casEN\"])\n",
    "    merge[\"NER_label\"] = merge[\"NER_label_spacy\"].combine_first(merge[\"NER_label_casEN\"])\n",
    "    merge[\"desc\"] = merge[\"desc_casEN\"].combine_first(merge[\"desc_spacy\"])\n",
    "    merge[\"file_id\"] = merge[\"file_id_casEN\"].combine_first(merge[\"file_id_spacy\"])\n",
    "    \n",
    "    merge = merge.sort_values(\n",
    "        by=[\"titles\"], \n",
    "        ascending=[False]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    merge[\"manual cat\"] = \"\"\n",
    "    merge[\"extent\"] = \"\"\n",
    "    merge[\"correct\"] = \"\"\n",
    "    merge[\"category\"] = \"\"\n",
    "\n",
    "    final_columns = ['manual cat', 'correct', 'extent', 'category','titles', 'NER', 'NER_label', 'desc', 'method',\n",
    "                     'main_graph', 'second_graph', 'third_graph', \"file_id\"]\n",
    "\n",
    "\n",
    "    merge = merge.drop_duplicates(subset=[\"titles\", \"NER\", \"NER_label\", \"method\", \"main_graph\", \"second_graph\", \"third_graph\"])\n",
    "\n",
    "    end_chrono = time.time()\n",
    "    chrono = end_chrono - start_chrono\n",
    "    print(f\"Merge CasEN & Spacy in : {chrono:.2f}s\")\n",
    "\n",
    "    return merge[final_columns]\n",
    "\n",
    "# intersection = merge_spacy_casEN(df_ner, df_casEN)\n",
    "# intersection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4feca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte into Excel File\n",
    "#intersection.to_excel(\"Result/xlsx/intersection.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961cbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspacy model :fr_core_news_sm\\nLoad data in : 1.42s\\nApply Spacy in : 75.02s\\nGenerate Spacy DataFrame in : 63.34s\\nGenerate text files in : 7.67s\\nGenerate CasEN DataFrame in : 81.52s\\nMerge CasEN & Spacy in : 0.86s\\nTotal Execute in : 960.58s\\n\\nCasEN : 265.93s\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "spacy model :fr_core_news_sm\n",
    "Load data in : 1.42s\n",
    "Apply Spacy in : 75.02s\n",
    "Generate Spacy DataFrame in : 63.34s\n",
    "Generate text files in : 7.67s\n",
    "Generate CasEN DataFrame in : 81.52s\n",
    "Merge CasEN & Spacy in : 0.86s\n",
    "Total Execute in : 960.58s\n",
    "\n",
    "CasEN : 730.75s\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0e2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pipeline_upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09eb0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy model :fr_core_news_sm\n",
      "Load data in : 5.51s\n",
      "Apply Spacy in : 71.66s\n",
      "Generate Spacy DataFrame in : 59.88s\n",
      "Generate text files in : 0.02s\n",
      "All.txt\n",
      "C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Result\\Corpus\t\t -> results in : C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Result\\CasEN_Result\\Res_CasEN_Analyse_synthese_grf\n",
      "1 files to process with CasEN in  C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Result\\Corpus\n",
      "C:\\Users\\valen\\AppData\\Local\\Unitex-GramLab\\App\\UnitexToolLogger.exe { BatchRunScript -e -i \"C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Result\\Corpus\" -o \"C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Result\\CasEN_Result\\Res_CasEN_Analyse_synthese_grf\" -t1 \"C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\CasEN_fr.2.0\\my_CasEN_lingpkg.zip\" -f -s \"script\\CasEN_Analyse_synthese_grf.uniscript\" }\n",
      "Built CasEN DataFrame in 3.20s\n",
      "Merge CasEN & Spacy in : 0.69s\n",
      "Total Execute in : 395.67s\n"
     ]
    }
   ],
   "source": [
    "# Spacy modeles\n",
    "small = \"fr_core_news_sm\"\n",
    "medium = \"fr_core_news_md\"\n",
    "large = \"fr_core_news_lg\"\n",
    "\n",
    "NER = spacy.load(small)\n",
    "data = \"ressources/20231101_raw.xlsx\"\n",
    "Pipeline_upgrade.Pipeline(data, \"Result/xlsx/intersection.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75097dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspacy model :fr_core_news_sm\\nLoad data in : 2.19s\\nApply Spacy in : 75.84s\\nGenerate Spacy DataFrame in : 61.66s\\nGenerate text files in : 0.02s\\nBuilt CasEN DataFrame in 2.78s\\nMerge CasEN & Spacy in : 0.50s\\nTotal Execute in : 408.92s\\n\\nCasEN : 265.93s\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "spacy model :fr_core_news_sm\n",
    "Load data in : 2.19s\n",
    "Apply Spacy in : 75.84s\n",
    "Generate Spacy DataFrame in : 61.66s\n",
    "Generate text files in : 0.02s\n",
    "Built CasEN DataFrame in 2.78s\n",
    "Merge CasEN & Spacy in : 0.50s\n",
    "Total Execute in : 408.92s\n",
    "\n",
    "CasEN : 265.93s\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa2c0813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    PB : en gardant que les \"graphes\" les plus grands, si jamais casEN trouve le prenom/nom et spacy que le nom alors il n\\'y a plus d\\'intersection\\n        sur le nom.\\n\\n        \\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    PB : en gardant que les \"graphes\" les plus grands, si jamais casEN trouve le prenom/nom et spacy que le nom alors il n'y a plus d'intersection\n",
    "        sur le nom.\n",
    "\n",
    "        \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa2a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the Excel file\n",
    "def correct_excel(df1 : pd.DataFrame, df2:pd.DataFrame):\n",
    "\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "\n",
    "    # CrÃ©er les clÃ©s dans les deux DataFrames\n",
    "    df1[\"key\"] = df1[[\"titles\", \"NER\", \"NER_label\", \"method\", \"hash\"]].apply(tuple, axis=1)\n",
    "    df2[\"key\"] = df2[[\"titles\", \"NER\", \"NER_label\", \"method\", \"file_id\"]].apply(tuple, axis=1)\n",
    "\n",
    "    # Colonnes Ã  transfÃ©rer\n",
    "    cols_to_copy = [\"manual cat\", \"correct\", \"extent\", \"category\"]\n",
    "\n",
    "    # CrÃ©er un dictionnaire : clÃ© -> valeurs des colonnes Ã  copier\n",
    "    df1_dict = df1.set_index(\"key\")[cols_to_copy].to_dict(orient=\"index\")\n",
    "\n",
    "    # Pour chaque ligne de df2, copier si la clÃ© existe\n",
    "    for col in cols_to_copy:\n",
    "        df2[col] = df2[\"key\"].map(lambda k: df1_dict.get(k, {}).get(col, \"\"))\n",
    "\n",
    "    # Supprimer la clÃ© temporaire\n",
    "    df2.drop(columns=[\"key\"], inplace=True)\n",
    "\n",
    "    return df2\n",
    "\n",
    "\n",
    "correction = pd.read_excel(\"ressources/20231101_Digital3D_Tele-Loisirs _telerama_NER_weekday_evaluation_v3(1).xlsx\")\n",
    "df_pipeline = pd.read_excel(\"Result/xlsx/intersection.xlsx\")\n",
    "\n",
    "corrected_pipeline = correct_excel(correction, df_pipeline)\n",
    "corrected_pipeline.to_excel(\"Result/xlsx/Final_result2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "671d55ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
