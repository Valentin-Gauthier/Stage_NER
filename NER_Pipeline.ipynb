{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8e22be",
   "metadata": {},
   "source": [
    "# ðŸ§  NER Pipeline\n",
    "This notebook demonstrates a complete Named Entity Recognition (NER) pipeline that combines results from two different sources: spaCy and CasEN. It handles everything from data loading and entity extraction to results merging and exporting.\n",
    "\n",
    "## ðŸ“„ Overview\n",
    "The pipeline:\n",
    "\n",
    "   - Loads input data from an Excel file.\n",
    "\n",
    "   - Applies NER using spaCy with the selected French model.\n",
    "\n",
    "   - Formats the data and exports text input for CasEN.\n",
    "\n",
    "   - Runs the CasEN engine on the generated text.\n",
    "\n",
    "   - Parses the CasEN output using BeautifulSoup and regex.\n",
    "\n",
    "   - Merges the results from both NER systems (spaCy + CasEN).\n",
    "\n",
    "   - Exports the merged results into a final Excel file.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ›  Dependencies\n",
    "\n",
    "To run this notebook, make sure you have the following:\n",
    "\n",
    "### âœ… Requirements\n",
    "\n",
    "- **Python 3.8 or higher**\n",
    "- **Jupyter Notebook** or **JupyterLab** (to run `.ipynb` files)\n",
    "\n",
    "Install Jupyter Notebook:\n",
    "```bash\n",
    "pip install notebook\n",
    "```\n",
    "### ðŸ“¦ Python Packages\n",
    "```bash\n",
    "pip install pandas spacy beautifulsoup4 openpyxl\n",
    "python -m spacy download fr_core_news_sm\n",
    "python -m spacy download fr_core_news_md\n",
    "python -m spacy download fr_core_news_lg\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c33ff51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- IMPORTS ----------------\n",
    "import time\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from spacy import displacy\n",
    "from time import sleep\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea0b2fc",
   "metadata": {},
   "source": [
    "# Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aafa1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_file(file_path:str):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        Load a Excel file into a DataFrame\n",
    "\n",
    "        Parameters:\n",
    "            file_path (str): The file path\n",
    "\n",
    "        Returns:\n",
    "            df (pd.Dataframe) : the result of Excel file\n",
    "    \"\"\"\n",
    "\n",
    "    # Start Chrono\n",
    "    start_chrono = time.time()\n",
    "\n",
    "    # Open the Excel file\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Clean\n",
    "    df.drop(df.columns[0], axis=1, inplace=True) # Drop the Column\n",
    "    #print(df.isnull().sum())\n",
    "    df = df.fillna('') # clean the missing description\n",
    "    \n",
    "    # End Chrono\n",
    "    end_chrono = time.time()\n",
    "    chrono = end_chrono - start_chrono\n",
    "    print(f\"Load data in : {chrono:.2f}s\")\n",
    "\n",
    "    return df\n",
    "\n",
    "#df = load_excel_file(\"ressources/20231101_raw.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428dfb15",
   "metadata": {},
   "source": [
    "# Use Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614f0a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed2cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the Spacy modele \n",
    "#NER = spacy.load(\"fr_core_news_sm\")\n",
    "#NER = spacy.load(\"fr_core_news_md\")\n",
    "#NER = spacy.load(\"fr_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2fc0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_spacy(df, NER):\n",
    "    \"\"\"\n",
    "        Apply Spacy\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.Dataframe): the dataframe \n",
    "    \"\"\"\n",
    "\n",
    "    ner_spacy = [] # (\"text_entite\", \"label\")\n",
    "    ner_spacy_id = [] # (\"start\", \"end\")\n",
    "\n",
    "    # Start Chrono\n",
    "    start_chrono = time.time()\n",
    "    # foreach row , use spacy on the description\n",
    "    for idx, row in df.iterrows():\n",
    "        desc = str(row[\"desc\"]) # extract the description\n",
    "        entities = NER(desc) # use spacy \n",
    "        entities_founds = set() # Manage duplicates automatically\n",
    "        entities_locations = []\n",
    "\n",
    "        for word in entities.ents:\n",
    "            entities_founds.add((word.text, word.label_))\n",
    "            entities_locations.append((word.start, word.end))\n",
    "\n",
    "        ner_spacy.append(entities_founds)\n",
    "        ner_spacy_id.append(entities_locations)\n",
    "\n",
    "    # End Chrono\n",
    "    end_chrono = time.time()\n",
    "    chrono = end_chrono - start_chrono\n",
    "    print(f\"Apply Spacy in : {chrono:.2f}s\")\n",
    "\n",
    "    return (ner_spacy, ner_spacy_id)\n",
    "\n",
    "# ner_spacy, ner_spacy_id = apply_spacy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158aa3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ner_dataframe(df, ner_spacy, ner_spacy_id, NER):\n",
    "    \"\"\"\n",
    "    Build a new DataFrame from the original DataFrame and precomputed NER outputs.\n",
    "    \n",
    "    The original DataFrame must contain at least two columns: 'titles' and 'desc'.\n",
    "    For each row, the function uses the provided NER results (entity text/label and their corresponding indices)\n",
    "    to extract a portion of the description text with some context around each entity.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The original DataFrame containing a 'titles' column and a 'desc' (description) column.\n",
    "        ner_spacy (list): A list (one per row) of lists of tuples, each tuple containing the entity text and its label.\n",
    "                          For example: [[(\"Barack Obama\", \"PERSON\"), (\"Washington\", \"GPE\")], ...]\n",
    "        ner_spacy_id (list): A list (one per row) of lists of tuples, each tuple containing the start and end indices of the entity.\n",
    "                             For example: [[(0, 12), (20, 30)], ...]\n",
    "                             \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the following columns:\n",
    "                      - \"titles\": the original title repeated for each extracted entity.\n",
    "                      - \"NER\": the extracted entity text.\n",
    "                      - \"NER_label\": the label/type of the entity.\n",
    "                      - \"desc\": a snippet of the original description around the entity.\n",
    "    \"\"\"\n",
    "\n",
    "    start_chrono = time.time()\n",
    "\n",
    "    # Convert the 'titles' and 'desc' columns to lists\n",
    "    titles_list = df[\"titles\"].tolist()\n",
    "    descriptions_list = df[\"desc\"].tolist()\n",
    "    \n",
    "    # Initialize lists to store the new data\n",
    "    title_list = []\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    location_list = []\n",
    "    file_id_list = []\n",
    "    \n",
    "    # Loop through each entry in the original DataFrame (by index)\n",
    "    for i in range(len(titles_list)):\n",
    "        current_desc = descriptions_list[i]\n",
    "        # Process the description using the NER function to obtain a full processed document.\n",
    "        # Assume that NER() returns an object where converting it to a string gives its full text.\n",
    "        current_NER_doc = NER(current_desc)\n",
    "        \n",
    "        # Get the precomputed NER entities and their indices for this row\n",
    "        current_NER = ner_spacy[i]\n",
    "        indices = ner_spacy_id[i]\n",
    "        \n",
    "        # Get the title associated with the current row\n",
    "        current_title = titles_list[i]\n",
    "        \n",
    "        # Loop over each entity and its corresponding indices\n",
    "        for entity, idx in zip(current_NER, indices):\n",
    "            entity_text, entity_label = entity\n",
    "            \n",
    "            # Append data to the corresponding list\n",
    "            title_list.append(current_title)\n",
    "            text_list.append(entity_text)\n",
    "            label_list.append(entity_label)\n",
    "            file_id_list.append(i)\n",
    "            \n",
    "            # Extract a snippet from the full document text around the entity for context.\n",
    "            # If the entity starts near the beginning, we extract from the start.\n",
    "            if idx[0] < 10:\n",
    "                # If entity is close to the end of the document, include all text until the end.\n",
    "                if idx[1] + 8 > len(current_NER_doc):\n",
    "                    location_list.append(current_NER_doc)\n",
    "                else:\n",
    "                    location_list.append(current_NER_doc[:idx[1] + 8])\n",
    "            else:\n",
    "                # Else extract 10 characters before the entity start.\n",
    "                # If the entity is near the end, extract until the end.\n",
    "                if idx[1] + 8 > len(current_NER_doc):\n",
    "                    location_list.append(current_NER_doc[idx[0] - 10:])\n",
    "                else:\n",
    "                    location_list.append(current_NER_doc[idx[0] - 10: idx[1] + 8])\n",
    "    \n",
    "    # Build a new DataFrame using the collected lists.\n",
    "    new_df = pd.DataFrame(zip(title_list, text_list, label_list, location_list, file_id_list))\n",
    "    \n",
    "    # Rename the columns to be more informative.\n",
    "    new_df.rename(columns={0: \"titles\", 1: \"NER\", 2: \"NER_label\", 3: \"desc\", 4:\"file_id\"}, inplace=True)\n",
    "\n",
    "    end_chrono = time.time()\n",
    "    chrono = end_chrono - start_chrono\n",
    "    print(f\"Generate Spacy DataFrame in : {chrono:.2f}s\")\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# df_ner = build_ner_dataframe(df, ner_spacy, ner_spacy_id)\n",
    "# df_ner.head()\n",
    "#df_ner.to_excel(\"Result/xlsx/DataFrame_Spacy.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9dd386",
   "metadata": {},
   "source": [
    "# CasEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d155ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_files(df):\n",
    "    start_chrono = time.time()\n",
    "\n",
    "    # Generate text file \n",
    "    text_location = \"Result/Corpus/All.txt\"\n",
    "    \n",
    "    with open(text_location, 'w', encoding=\"utf-8\") as f:\n",
    "        for idx, desc in enumerate(df[\"desc\"]):\n",
    "            f.write(f'<doc id=\"{idx}\">')\n",
    "            f.write(str(desc))\n",
    "            f.write('</doc>\\n')\n",
    "\n",
    "    end_chrono = time.time()\n",
    "    chrono = end_chrono - start_chrono\n",
    "    print(f\"Generate text files in : {chrono:.2f}s\")\n",
    "\n",
    "#generate_text_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab275297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CasEN\n",
    "# casen_ipynb_path = \"../CasEN_fr.2.0/CasEN.ipynb\"\n",
    "# get_ipython().run_line_magic('run', str(casen_ipynb_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17275a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_tree(soup_doc: BeautifulSoup, doc_id: int):\n",
    "    \"\"\"\n",
    "    Ne remonte que les entitÃ©s ayant un grf et sans ancÃªtre grf,\n",
    "    et stocke pour chacune :\n",
    "      - file_id, tag, text, grf (main)\n",
    "      - second_graph  = grf du 1er enfant direct\n",
    "      - third_graph   = grf du 2Ã¨me enfant direct\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    excluded = {\"s\", \"p\", \"doc\"}\n",
    "\n",
    "    # On parcourt tous les Ã©lÃ©ments taggÃ©s avec grf\n",
    "    for element in soup_doc.find_all(lambda tag: tag.name not in excluded and tag.has_attr(\"grf\")):\n",
    "        # Si un des ancÃªtres a aussi un grf, on l'ignore (c'est un sous-Ã©lÃ©ment)\n",
    "        if any(ancestor.has_attr(\"grf\") for ancestor in element.parents if ancestor.name not in excluded):\n",
    "            continue\n",
    "\n",
    "        main_grf = element[\"grf\"]\n",
    "        # on collecte les grf des enfants directs pour second et third\n",
    "        children = [child for child in element.find_all(recursive=False) if child.has_attr(\"grf\")]\n",
    "        second = children[0][\"grf\"] if len(children) >= 1 else \"\"\n",
    "        third  = children[1][\"grf\"] if len(children) >= 2 else \"\"\n",
    "\n",
    "        entities.append({\n",
    "            \"file_id\":       doc_id,\n",
    "            \"tag\":           element.name,\n",
    "            \"text\":          element.get_text(),\n",
    "            \"grf\":           main_grf,\n",
    "            \"second_graph\":  second,\n",
    "            \"third_graph\":   third\n",
    "        })\n",
    "\n",
    "    return entities\n",
    "\n",
    "def extract_entities_from_file(result_path :str):\n",
    "\n",
    "    with open(result_path, 'r', encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    content = re.sub(r'</?s\\b[^>]*>', '', content)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    all_entities = []\n",
    "\n",
    "    for doc in soup.find_all('doc'):\n",
    "        doc_id = int(doc.attrs.get(\"id\"))\n",
    "        ents = extract_entities_from_tree(doc, doc_id)\n",
    "        all_entities.extend(ents)\n",
    "\n",
    "    return all_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acab817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_context(full_desc: str, ner_text: str, window: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a context window around the first match of the NER text in the full descriptionE\n",
    "    \n",
    "    Parameters:\n",
    "        full_desc (str): The full text description.\n",
    "        ner_text (str): The NER text to find in the description.\n",
    "        window (int): Number of words to extract before and after the match\n",
    "\n",
    "    Returns:\n",
    "        str: A snippet of the description centered around the NER match\n",
    "    \"\"\"\n",
    "    # Normalize input for search\n",
    "    full_desc_clean = re.sub(r'[^\\w\\s]', '', full_desc.lower())\n",
    "    ner_clean = re.sub(r'[^\\w\\s]', '', ner_text.lower())\n",
    "\n",
    "    # Tokenize words\n",
    "    words = full_desc.split()\n",
    "    clean_words = full_desc_clean.split()\n",
    "    ner_words = ner_clean.split()\n",
    "\n",
    "    # Try to find full match of all NER words\n",
    "    for i in range(len(clean_words) - len(ner_words) + 1):\n",
    "        if clean_words[i:i+len(ner_words)] == ner_words:\n",
    "            start = max(0, i - window)\n",
    "            end = min(len(words), i + len(ner_words) + window)\n",
    "            return ' '.join(words[start:end])\n",
    "\n",
    "    return full_desc\n",
    "\n",
    "def find_ner_label(tag:str) -> str:\n",
    "    if tag in [\"persname\", \"surname\", \"forename\"]:\n",
    "        return \"PER\"\n",
    "    elif tag in ['placename','geoname', 'adress', 'adrline']:\n",
    "        return \"LOC\"\n",
    "    elif tag in [\"orgname\", \"org\"]:\n",
    "        return \"ORG\"\n",
    "    else:\n",
    "        return \"MISC\"\n",
    "\n",
    "def casEN_df_single(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a DataFrame of NER results from a single CasEN output file.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    list_entities = extract_entities_from_file(\"Result/CasEN_Result/Res_CasEN_Analyse_synthese_grf/All.result.txt\")\n",
    "    rows = []\n",
    "    for entity in list_entities:\n",
    "        idx = entity[\"file_id\"]\n",
    "        ner_text = entity[\"text\"]\n",
    "        context = get_ner_context(df.loc[idx, \"desc\"], ner_text, window=7)\n",
    "        rows.append({\n",
    "            \"titles\": df.loc[idx, \"titles\"],\n",
    "            \"NER\": ner_text,\n",
    "            \"NER_label\": find_ner_label(entity[\"tag\"]),\n",
    "            \"desc\": context,\n",
    "            \"method\": \"CasEN\",\n",
    "            \"main_graph\": entity[\"grf\"],\n",
    "            \"second_graph\": entity.get(\"second_graph\", \"\"),\n",
    "            \"third_graph\": entity.get(\"third_graph\", \"\"),\n",
    "            \"file_id\": idx\n",
    "        })\n",
    "    df_casen = pd.DataFrame(rows)\n",
    "    print(f\"Built CasEN DataFrame in {time.time() - start:.2f}s\")\n",
    "    return df_casen\n",
    "#df_casEN = casEN_df_single(df)\n",
    "#df_casEN.to_excel(\"Result/xlsx/DataFrame_CasEN.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "436f1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_method(row):\n",
    "    if row['_merge'] == 'both':\n",
    "        return \"intersection\"\n",
    "    elif row['_merge'] == 'left_only':\n",
    "        return row['method_spacy']\n",
    "    elif row['_merge'] == 'right_only':\n",
    "        return row['method_casEN']\n",
    "    else:\n",
    "        return \"Spacy\"\n",
    "\n",
    "def merge_spacy_casEN(df_spacy:pd.DataFrame,df_casEN:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    start_chrono = time.time()\n",
    "\n",
    "    if \"method\" not in df_spacy.columns:\n",
    "        df_spacy[\"method\"] = \"Spacy\"\n",
    "\n",
    "    # Create unique key on (\"titles\", \"NER\", \"NER_label\")\n",
    "    df_spacy[\"key\"] = df_spacy[[\"titles\", \"NER\", \"NER_label\", \"file_id\"]].apply(lambda x: tuple(x), axis=1)\n",
    "    df_casEN[\"key\"] = df_casEN[[\"titles\", \"NER\", \"NER_label\", \"file_id\"]].apply(lambda x: tuple(x), axis=1)\n",
    "\n",
    "    # merge dataframs on the key\n",
    "    merge = pd.merge(df_spacy, df_casEN, on='key', how='outer',suffixes=('_spacy', '_casEN'), indicator=True)\n",
    "\n",
    "    # fix the shared columns\n",
    "    merge[\"titles\"] = merge[\"titles_spacy\"].combine_first(merge[\"titles_casEN\"])\n",
    "    merge['method'] = merge.apply(determine_method, axis=1)\n",
    "    merge[\"NER\"] = merge[\"NER_spacy\"].combine_first(merge[\"NER_casEN\"])\n",
    "    merge[\"NER_label\"] = merge[\"NER_label_spacy\"].combine_first(merge[\"NER_label_casEN\"])\n",
    "    merge[\"desc\"] = merge[\"desc_casEN\"].combine_first(merge[\"desc_spacy\"])\n",
    "    merge[\"file_id\"] = merge[\"file_id_casEN\"].combine_first(merge[\"file_id_spacy\"])\n",
    "    \n",
    "    merge = merge.sort_values(\n",
    "        by=[\"titles\"], \n",
    "        ascending=[False]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    merge[\"manual cat\"] = \"\"\n",
    "    merge[\"extent\"] = \"\"\n",
    "    merge[\"correct\"] = \"\"\n",
    "    merge[\"category\"] = \"\"\n",
    "\n",
    "    final_columns = ['manual cat', 'correct', 'extent', 'category','titles', 'NER', 'NER_label', 'desc', 'method',\n",
    "                     'main_graph', 'second_graph', 'third_graph', \"file_id\"]\n",
    "\n",
    "\n",
    "    merge = merge.drop_duplicates(subset=[\"titles\", \"NER\", \"NER_label\", \"method\", \"main_graph\", \"second_graph\", \"third_graph\"])\n",
    "\n",
    "    end_chrono = time.time()\n",
    "    chrono = end_chrono - start_chrono\n",
    "    print(f\"Merge CasEN & Spacy in : {chrono:.2f}s\")\n",
    "\n",
    "    return merge[final_columns]\n",
    "\n",
    "# intersection = merge_spacy_casEN(df_ner, df_casEN)\n",
    "# intersection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4feca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte into Excel File\n",
    "#intersection.to_excel(\"Result/xlsx/intersection.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f961cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spacy model :fr_core_news_sm\n",
    "# Load data in : 1.42s\n",
    "# Apply Spacy in : 75.02s\n",
    "# Generate Spacy DataFrame in : 63.34s\n",
    "# Generate text files in : 7.67s\n",
    "# Generate CasEN DataFrame in : 81.52s\n",
    "# Merge CasEN & Spacy in : 0.86s\n",
    "# Total Execute in : 960.58s\n",
    "\n",
    "# CasEN : 730.75s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a0e2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pipeline_upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a09eb0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy modeles\n",
    "small = \"fr_core_news_sm\"\n",
    "medium = \"fr_core_news_md\"\n",
    "large = \"fr_core_news_lg\"\n",
    "\n",
    "#data = \"ressources/20231101_raw.xlsx\"\n",
    "#Pipeline_upgrade.Pipeline(data, \"Result/xlsx/intersection_large.xlsx\", large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75097dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy model :fr_core_news_sm\n",
    "# Load data in : 2.19s\n",
    "# Apply Spacy in : 75.84s\n",
    "# Generate Spacy DataFrame in : 61.66s\n",
    "# Generate text files in : 0.02s\n",
    "# Built CasEN DataFrame in 2.78s\n",
    "# Merge CasEN & Spacy in : 0.50s\n",
    "# Total Execute in : 408.92s\n",
    "\n",
    "# CasEN : 265.93s\n",
    "\n",
    "\n",
    "# Load data in : 1.91s\n",
    "# Loaded spaCy model: core_news_lg\n",
    "# Word vectors shape: (500000, 300)\n",
    "# Apply Spacy in : 75.82s\n",
    "# Generate Spacy DataFrame in : 70.91s\n",
    "# Generate text files in : 0.03s\n",
    "# All.txt\n",
    "# Built CasEN DataFrame in 3.23s\n",
    "# Merge CasEN & Spacy in : 0.65s\n",
    "# Total Execute in : 453.31s\n",
    "\n",
    "# CasEN : 300.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa2c0813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    PB : en gardant que les \"graphes\" les plus grands, si jamais casEN trouve le prenom/nom et spacy que le nom alors il n\\'y a plus d\\'intersection\\n        sur le nom.\\n\\n        \\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    PB : en gardant que les \"graphes\" les plus grands, si jamais casEN trouve le prenom/nom et spacy que le nom alors il n'y a plus d'intersection\n",
    "        sur le nom.\n",
    "\n",
    "        \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259b6ff",
   "metadata": {},
   "source": [
    "### Correct the Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52fa2a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the Excel file\n",
    "def correct_excel(df1 : pd.DataFrame, df2:pd.DataFrame):\n",
    "\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "\n",
    "    # CrÃ©er les clÃ©s dans les deux DataFrames\n",
    "    df1[\"key\"] = df1[[\"titles\", \"NER\", \"NER_label\", \"method\", \"hash\"]].apply(tuple, axis=1)\n",
    "    df2[\"key\"] = df2[[\"titles\", \"NER\", \"NER_label\", \"method\", \"file_id\"]].apply(tuple, axis=1)\n",
    "\n",
    "    # Colonnes Ã  transfÃ©rer\n",
    "    cols_to_copy = [\"manual cat\", \"correct\", \"extent\", \"category\"]\n",
    "\n",
    "    # CrÃ©er un dictionnaire : clÃ© -> valeurs des colonnes Ã  copier\n",
    "    df1_dict = df1.set_index(\"key\")[cols_to_copy].to_dict(orient=\"index\")\n",
    "\n",
    "    # Pour chaque ligne de df2, copier si la clÃ© existe\n",
    "    for col in cols_to_copy:\n",
    "        df2[col] = df2[\"key\"].map(lambda k: df1_dict.get(k, {}).get(col, \"\"))\n",
    "\n",
    "    # Supprimer la clÃ© temporaire\n",
    "    df2.drop(columns=[\"key\"], inplace=True)\n",
    "\n",
    "    return df2\n",
    "\n",
    "\n",
    "# correction = pd.read_excel(\"ressources/20231101_Digital3D_Tele-Loisirs _telerama_NER_weekday_evaluation_v3(1).xlsx\")\n",
    "# df_pipeline = pd.read_excel(\"Result/xlsx/intersection_large.xlsx\")\n",
    "\n",
    "# corrected_pipeline = correct_excel(correction, df_pipeline)\n",
    "# corrected_pipeline.to_excel(\"Result/xlsx/Final_result_large.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_excel_data in : 1.81s\n",
      "load_spaCy in : 3.88s\n",
      "apply_spacy in : 75.46s\n",
      "build_spaCy_df in : 62.47s\n",
      "removed corpus.txt from Corpus\n",
      "generate_text_file in : 0.00s\n",
      "removed corpus.result.txt from CasEN result\n",
      "corpus.txt\n",
      "C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Result\\Corpus\t\t -> results in : C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Result\\CasEN_Result\\Res_CasEN_Analyse_synthese_grf\n",
      "1 files to process with CasEN in  C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Result\\Corpus\n",
      "C:\\Users\\valen\\AppData\\Local\\Unitex-GramLab\\App\\UnitexToolLogger.exe { BatchRunScript -e -i \"C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Result\\Corpus\" -o \"C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Result\\CasEN_Result\\Res_CasEN_Analyse_synthese_grf\" -t1 \"C:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\CasEN_fr.2.0\\my_CasEN_lingpkg.zip\" -f -s \"script\\CasEN_Analyse_synthese_grf.uniscript\" }\n",
      "extract_entities_from_file in : 2.38s\n",
      "build_casEN_df in : 3.30s\n",
      "merge_spacy_casEN in : 0.68s\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 26\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Pipeline Settings\u001b[39;00m\n\u001b[0;32m     10\u001b[0m P \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[0;32m     11\u001b[0m             Excel_raw_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mressources/20231101_raw.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     12\u001b[0m             text_column_name\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m             correction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mressources/20231101_Digital3D_Tele-Loisirs _telerama_NER_weekday_evaluation_v3(1).xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m             )\n\u001b[1;32m---> 26\u001b[0m \u001b[43mP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Pipeline_upgraded.py:47\u001b[0m, in \u001b[0;36mPipeline.chrono.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer_log:\n\u001b[0;32m     46\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 47\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer_log:\n\u001b[0;32m     49\u001b[0m     duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Pipeline_upgraded.py:400\u001b[0m, in \u001b[0;36mPipeline.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_spacy_casEN()\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_correction: \u001b[38;5;66;03m# apply correction\u001b[39;00m\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult_location, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Pipeline_upgraded.py:47\u001b[0m, in \u001b[0;36mPipeline.chrono.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer_log:\n\u001b[0;32m     46\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 47\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer_log:\n\u001b[0;32m     49\u001b[0m     duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\valen\\Documents\\Informatique-L3\\Stage\\Stage\\Pipeline_upgraded.py:361\u001b[0m, in \u001b[0;36mPipeline.correct_excel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;129m@chrono\u001b[39m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcorrect_excel\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    359\u001b[0m \n\u001b[0;32m    360\u001b[0m     \u001b[38;5;66;03m# CrÃ©er les clÃ©s dans les deux DataFrames\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorrection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrection\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNER_label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mtuple\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitles\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNER\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNER_label\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mtuple\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# Colonnes Ã  transfÃ©rer\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'list'"
     ]
    }
   ],
   "source": [
    "from Pipeline_upgraded import Pipeline\n",
    "\n",
    "# spaCy model\n",
    "small = \"fr_core_news_sm\"\n",
    "medium = \"fr_core_news_md\"\n",
    "large = \"fr_core_news_lg\"\n",
    "\n",
    "# Pipeline Settings\n",
    "\n",
    "#  Run the pipeline and generate Excel result\n",
    "\n",
    "#   Parameters:\n",
    "#      Excel_raw_data (str): Path for the Excel file containing text to analyse\n",
    "#      text_column_name (str): the name of the column wich contains the description to analyse\n",
    "#      spacy_model (str) : the name of the spaCy model to load\n",
    "#      result_location (str): the destination path for save the Excel file result\n",
    "#      casEN_ipynb_location (str): the path to the casEN notebook for run casEN\n",
    "#      CasEN_corpus_folder (str) : the folder containing the corpus text for casEN\n",
    "#      CasEN_result_folder (str) : the folder containing the result of CasEN analyse\n",
    "#      timer (bool) : show the time of every duration\n",
    "#      timer_log (bool): save in the log file\n",
    "#      log_location (str): the location of the log.txt\n",
    "#      multiple_file_generated (bool): on True , generate multiple file for CasEN, on False only one file. \n",
    "#      auto_correction (bool) : try to match the correction\n",
    "#      correction (str) : the path to the excel corrected\n",
    "\n",
    "\n",
    "P = Pipeline(\n",
    "            Excel_raw_data=\"ressources/20231101_raw.xlsx\", \n",
    "            text_column_name= \"desc\",\n",
    "            spacy_model=small, \n",
    "            result_location=\"Result/xlsx/Pipeline.xlsx\",\n",
    "            casEN_ipynb_location=\"../CasEN_fr.2.0/CasEN.ipynb\", \n",
    "            CasEN_corpus_folder=\"Result/Corpus\", \n",
    "            CasEN_result_folder=\"Result/CasEN_Result/Res_CasEN_Analyse_synthese_grf\",\n",
    "            timer=True, \n",
    "            timer_log=True, \n",
    "            log_location=\"Result/log\",\n",
    "            multiple_file_generated=False, \n",
    "            auto_correction=True, \n",
    "            correction_path=\"ressources/20231101_Digital3D_Tele-Loisirs _telerama_NER_weekday_evaluation_v3(1).xlsx\"\n",
    "            )\n",
    "\n",
    "P.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
